{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arc fetching\n",
    "\n",
    "This notebook is fetching the arcs of the series. An arc is a number of episodes that together make up an arc of the series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by important the neccesary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with help from BeautilfulSoup, a function will be defined to fetch the arc names from the Naruto fandom page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_arc_names(url):\n",
    "    \"\"\"\n",
    "    Scrape arc names from a given URL.\n",
    "\n",
    "    Parameters:\n",
    "    - url (str): The URL of the page containing arc names.\n",
    "\n",
    "    Returns:\n",
    "    - names_list (list): A list of cleaned and formatted arc names.\n",
    "\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        category_members = soup.find('div', {'class': 'category-page__members'})\n",
    "        all_content = \"\"\n",
    "\n",
    "        if category_members:\n",
    "            content = category_members.get_text()\n",
    "            all_content += content\n",
    "        else:\n",
    "            print(\"Div with class 'category-page__members' not found on the page.\")\n",
    "    else:\n",
    "        print(\"Failed to retrieve the page. Status code:\", response.status_code)\n",
    "\n",
    "    # Clean and preprocess the extracted string\n",
    "    cleaned_string = re.sub(r'.\\t', '', all_content.replace('\\n', ' ')).replace('\\t', ' ')\n",
    "\n",
    "    # Split the input string by two or more whitespace characters using regular expression\n",
    "    name_list = re.split(r'\\s{2,}', cleaned_string)\n",
    "\n",
    "    name_list = [name.strip() for name in name_list if name.strip()]\n",
    "    names_list = [name.replace(' ', '_') for name in name_list]\n",
    "\n",
    "    return names_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function is called on the URL containing the wanted arc names and is stored into a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Plot_of_Naruto', 'Academy_Entrance_Arc', 'Akatsuki_Suppression_Mission', 'Ao_Arc', 'Bikōchū_Search_Mission', \"Birth_of_the_Ten-Tails'_Jinchūriki\", \"Boruto's_Return_Arc\", 'Buried_Gold_Excavation_Mission', 'Byakuya_Gang_Arc', 'Childhood', 'Chūnin_Exams_(Arc)', 'Chōchō_Arc', 'Chūnin_Re-Examination_Arc', \"Code's_Assault_Arc\", 'Cursed_Warrior_Extermination_Mission', 'Kurosuki_Family_Removal_Mission', 'Kaguya_Ōtsutsuki_Strikes', 'Kaima_Capture_Mission', 'Kakashi_Gaiden', \"Kakashi's_Anbu_Arc:_The_Shinobi_That_Lives_in_the_Darkness\", 'Kara_Actuation_Arc', 'Kawaki_&_Himawari_Academy_Arc', 'Kawaki_Arc', 'Kazekage_Rescue_Mission', 'Konoha_Crush_(Arc)', 'Konoha_Hiden:_The_Perfect_Day_for_a_Wedding_(Arc)', 'Konoha_Plans_Recapture_Mission', \"Konohamaru's_Love_Arc\", 'Fated_Battle_Between_Brothers', 'Five_Kage_Summit_(Arc)', 'Fourth_Shinobi_World_War:_Climax', 'Fourth_Shinobi_World_War:_Confrontation', 'Fourth_Shinobi_World_War:_Countdown', 'Gantetsu_Escort_Mission', 'Genin_Mission_Arc', 'Gosunkugi_Capture_Mission', 'Graduation_Exams_Arc', 'Great_Sea_Battle_of_Kirigakure_Arc', \"In_Naruto's_Footsteps:_The_Friends'_Paths\", 'Itachi_Pursuit_Mission', 'Itachi_Shinden_Book:_Light_and_Darkness', 'Jiraiya_Shinobi_Handbook:_The_Tale_of_Naruto_the_Hero', 'Jūgo_Arc', 'Labyrinth_Game_Arc', 'Land_of_Rice_Fields_Investigation_Mission', 'Land_of_Tea_Escort_Mission', 'Menma_Memory_Search_Mission', \"Mitsuki's_Disappearance_Arc\", 'Mizuki_Tracking_Mission', 'Mujina_Bandits_Arc', 'Naruto_Gaiden:_The_Seventh_Hokage_and_the_Scarlet_Spring', 'Omnipotence_Arc', 'One-Tail_Escort_Arc', \"Pain's_Assault_(Arc)\", 'Paradise_Life_on_a_Boat', 'Parent_and_Child_Day_Arc', 'Past_Arc:_The_Locus_of_Konoha', 'Peddlers_Escort_Mission', 'Power_(Arc)', 'Prologue_—_Land_of_Waves', 'Sasuke_Recovery_Mission', 'Sasuke_Retsuden_Arc', 'Sasuke_Retsuden:_The_Uchiha_Descendants_and_the_Heavenly_Stardust_(manga)', 'Sasuke_Shinden:_Book_of_Sunrise_(Arc)', 'School_Trip_Arc', 'Search_for_Tsunade', 'Shikamaru_Hiden:_A_Cloud_Drifting_in_Silent_Darkness_(Arc)', 'Six-Tails_Unleashed', 'Star_Guard_Mission', 'Steam_Ninja_Scrolls_Arc', 'Sunagakure_Support_Mission', 'Tale_of_Jiraiya_the_Gallant', 'Tenchi_Bridge_Reconnaissance_Mission', 'Third_Great_Beast_Arc', \"Three-Tails'_Appearance\", 'Time_Slip_Arc', 'Twelve_Guardian_Ninja_(Arc)', 'Versus_Momoshiki_Arc', 'Yakumo_Kurama_Rescue_Mission']\n"
     ]
    }
   ],
   "source": [
    "arc_url = 'https://naruto.fandom.com/wiki/Category:Arcs'\n",
    "arc_names_list = scrape_arc_names(arc_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, a function to fetch each episode name within each arc is defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_arc_episodes(url):\n",
    "    \"\"\"\n",
    "    Scrape arc episodes data from a given URL.\n",
    "\n",
    "    Parameters:\n",
    "    - url (str): The URL of the page containing arc episodes data.\n",
    "\n",
    "    Returns:\n",
    "    - episode_data (list of dict): A list of dictionaries containing episode data (episode number and title).\n",
    "\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        title_span = soup.find('span', class_='mw-headline', string='Episodes')\n",
    "\n",
    "        if title_span:\n",
    "    \n",
    "            table = title_span.find_next('table', class_='box table colored bordered innerbordered style-basic')\n",
    "\n",
    "            if table:\n",
    "                episode_data = []\n",
    "\n",
    "                for row in table.find_all('tr')[1:]:  # Skip the header row\n",
    "                    columns = row.find_all('td')\n",
    "\n",
    "                    episode_number = row.find('th').get_text(strip=True)\n",
    "\n",
    "                    episode_title_tag = columns[0].find('a')\n",
    "                    episode_title = episode_title_tag.get_text(strip=True).replace(' ', '_') if episode_title_tag else None\n",
    "\n",
    "                    episode_data.append({\n",
    "                        'Episode Number': episode_number,\n",
    "                        'Episode Title': episode_title,\n",
    "                    })\n",
    "\n",
    "                return episode_data\n",
    "\n",
    "            else:\n",
    "                print(\"Table not found after the title 'Episode'.\")\n",
    "                return None\n",
    "\n",
    "        else:\n",
    "            print(\"Title with text 'Episode' not found on the page.\")\n",
    "            return None\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the function is called in order to retrieve the episode names within each arc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title with text 'Episode' not found on the page.\n",
      "Failed to retrieve episode data for the arc Plot_of_Naruto\n",
      "Title with text 'Episode' not found on the page.\n",
      "Failed to retrieve episode data for the arc Boruto's_Return_Arc\n",
      "Title with text 'Episode' not found on the page.\n",
      "Failed to retrieve episode data for the arc Omnipotence_Arc\n",
      "Title with text 'Episode' not found on the page.\n",
      "Failed to retrieve episode data for the arc Sasuke_Retsuden:_The_Uchiha_Descendants_and_the_Heavenly_Stardust_(manga)\n"
     ]
    }
   ],
   "source": [
    "# Base URL for the Naruto Fandom website\n",
    "url_base = 'https://naruto.fandom.com/wiki/'\n",
    "\n",
    "arc_episodes_dict = {}\n",
    "\n",
    "for arc in arc_names_list:    \n",
    "    url = url_base + arc\n",
    "    arc_episodes = scrape_arc_episodes(url)\n",
    "    \n",
    "    if arc_episodes:\n",
    "        arc_episodes_dict[arc] = arc_episodes\n",
    "    else:\n",
    "        print(f'Failed to retrieve episode data for the arc {arc}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, the data is stored as a json file so that it can be used for analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save arc episodes data to json file\n",
    "with open('./data/arc_episodes_data.json', 'w') as json_file:\n",
    "    json.dump(arc_episodes_dict, json_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
