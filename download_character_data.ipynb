{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import json\n",
    "from aiohttp import hdrs\n",
    "from aiohttp import ClientResponseError, TooManyRedirects\n",
    "from yarl import URL  # Import the URL class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved combined content to narutopedia_combined_content.txt\n"
     ]
    }
   ],
   "source": [
    "# Base URL for Narutopedia character category\n",
    "url_base = 'https://naruto.fandom.com/wiki/Category:Characters{}'\n",
    "query_list = [\n",
    "    '', \n",
    "    '?from=Eiki+Fūma%0AEiki+Fūma', \n",
    "    '?from=Hidari%0AHidari', \n",
    "    '?from=Karai%0AKarai', \n",
    "    '?from=Matsuba%0AMatsuba', \n",
    "    '?from=Rikumaru%0ARikumaru', \n",
    "    '?from=Taiki%0ATaiki', \n",
    "    '?from=Yubina%0AYubina'\n",
    "]\n",
    "\n",
    "# Create an empty string to store the concatenated content\n",
    "all_content = \"\"\n",
    "\n",
    "for query in query_list:\n",
    "    # Construct the URL for the current query\n",
    "    url = url_base.format(query)\n",
    "    \n",
    "    # Send an HTTP GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find the element with class 'category-page__members'\n",
    "        category_members = soup.find('div', {'class': 'category-page__members'})\n",
    "\n",
    "        # Extract the content within the 'category-page__members' div\n",
    "        if category_members:\n",
    "            content = category_members.get_text()\n",
    "            # Append the content to the all_content string\n",
    "            all_content += content\n",
    "        else:\n",
    "            print(\"Div with class 'category-page__members' not found on the page.\")\n",
    "    else:\n",
    "        print(\"Failed to retrieve the page. Status code:\", response.status_code)\n",
    "\n",
    "# Save the concatenated content to a single text file\n",
    "with open('narutopedia_combined_content.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(all_content)\n",
    "    print(\"Saved combined content to narutopedia_combined_content.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_string = re.sub(r'.\\t', '', all_content.replace('\\n', ' ')).replace('\\t', ' ')\n",
    "\n",
    "# Split the input string by two or more whitespace characters using regular expression\n",
    "name_list = re.split(r'\\s{2,}', cleaned_string)\n",
    "\n",
    "# Filter out any empty strings\n",
    "name_list = [name.strip() for name in name_list if name.strip()]\n",
    "\n",
    "names_list = []\n",
    "for name in name_list:\n",
    "    name=name.replace(' ', '_')\n",
    "    names_list.append(name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_characters = 'https://naruto.fandom.com/wiki/{}'\n",
    "\n",
    "characters_texts = {}\n",
    "characters_links = {}\n",
    "\n",
    "\n",
    "async def fetch_character_data(name, session):\n",
    "\n",
    "    url_character = url_characters.format(name)\n",
    "    \n",
    "    async with session.get(url_character) as response:\n",
    "        if response.status == 200:\n",
    "            html_content = await response.text()\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "            \n",
    "            category_members = soup.find_all('p')\n",
    "            all_links = [tag['href'].replace('/wiki/', '') for tag in soup.select('p a[href]')]\n",
    "\n",
    "            characters_links[name] = all_links\n",
    "\n",
    "            for data in category_members:\n",
    "                characters_texts[name] = data.get_text()\n",
    "\n",
    "\n",
    "async def main():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [fetch_character_data(name, session) for name in names_list]\n",
    "        await asyncio.gather(*tasks)\n",
    "\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No infobox found for Zetsu\n"
     ]
    }
   ],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url_base = 'https://naruto.fandom.com/wiki/{}'\n",
    "characters_infobox = {}\n",
    "\n",
    "\n",
    "async def fetch_infobox_data(name, session):\n",
    "    \n",
    "    url = url_base.format(name)\n",
    "\n",
    "    async with session.get(url) as response:\n",
    "        if response.status == 200:\n",
    "            html_content = await response.text()\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "            infobox_table = soup.find('table', class_='infobox')\n",
    "            if infobox_table:\n",
    "                keys = []\n",
    "                values = []\n",
    "                capture_info = False\n",
    "\n",
    "                for row in infobox_table.find_all('tr'):\n",
    "                    th = row.find('th')\n",
    "                    td = row.find('td')\n",
    "\n",
    "                    if th and 'Personal' in th.get_text():\n",
    "                        capture_info = True\n",
    "                        continue\n",
    "\n",
    "                    if capture_info:\n",
    "                        key = th.get_text(strip=True) if th else None\n",
    "                        value = td.get_text(strip=True) if td else None\n",
    "\n",
    "                        if td:\n",
    "                            ul_values = []\n",
    "                            for ul in td.find_all('ul', recursive=False):\n",
    "                                li_values = [li.get_text(strip=True) for li in ul.find_all('li')]\n",
    "                                ul_values.extend(li_values)\n",
    "\n",
    "                            if ul_values:\n",
    "                                key = key if key else \"Additional Information\"\n",
    "                                keys.append(key)\n",
    "                                values.append(ul_values)\n",
    "                                continue\n",
    "\n",
    "                        if th and 'mainheader' in th.get('class', []):\n",
    "                            next_tr = row.find_next('tr')\n",
    "                            if next_tr:\n",
    "                                value = next_tr.get_text(strip=True)\n",
    "\n",
    "                        if key and value:\n",
    "                            keys.append(key)\n",
    "                            values.append(value)\n",
    "\n",
    "                result_dict = dict(zip(keys, values))\n",
    "                characters_infobox[name] = result_dict\n",
    "            else:\n",
    "                print(f\"No infobox found for {name}\")\n",
    "        else:\n",
    "            print(f\"Failed to fetch data for {name}. Status code: {response.status}\")\n",
    "\n",
    "\n",
    "async def main_2():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [fetch_infobox_data(name, session) for name in names_list]\n",
    "        await asyncio.gather(*tasks)\n",
    "\n",
    "await main_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dictionary to a JSON file\n",
    "with open('./data/characters_list.json', 'w') as json_file:\n",
    "    json.dump(names_list, json_file)\n",
    "\n",
    "with open('./data/characters_texts.json', 'w') as json_file:\n",
    "    json.dump(characters_texts, json_file)\n",
    "\n",
    "with open('./data/characters_links.json', 'w') as json_file:\n",
    "    json.dump(characters_links, json_file)\n",
    "\n",
    "with open('./data/characters_infobox.json', 'w') as json_file:\n",
    "    json.dump(characters_infobox, json_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_characters = 'https://naruto.fandom.com/wiki/{}'\n",
    "\n",
    "async def fetch_link_name(key, values, session, counter, progress_callback):\n",
    "    header_names = []  # Store header names for each key\n",
    "    for idx, value in enumerate(values, start=counter + 1):\n",
    "        url_character = url_characters.format(value)\n",
    "        try:\n",
    "            async with session.get(url_character) as response:\n",
    "                if response.status == 200:\n",
    "                    html_content = await response.text()\n",
    "                    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "                    # Extract the header name\n",
    "                    header_name = soup.find('h1', {'class': 'page-header__title'})\n",
    "                    name = header_name.text.strip() if header_name else f\"No header found for {value}\"\n",
    "                    header_names.append(name)\n",
    "\n",
    "                    # Check and notify the progress callback every 100 fetches\n",
    "                    if idx % 100 == 0:\n",
    "                        progress_callback(key, idx)\n",
    "\n",
    "        except TooManyRedirects as e:\n",
    "            print(f\"TooManyRedirects exception for {url_character}: {e}\")\n",
    "            continue\n",
    "        except ClientResponseError as e:\n",
    "            print(f\"ClientResponseError exception for {url_character}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Update the original dictionary with the fetched header names\n",
    "    characters_links[key] = header_names\n",
    "    return len(header_names)\n",
    "\n",
    "\n",
    "async def process_batch(keys, session, counter, progress_callback):\n",
    "    tasks = [fetch_link_name(key, values, session, counter, progress_callback) for key, values in characters_links.items() if key in keys]\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "def print_progress(key, count):\n",
    "    print(f\"Fetched {count} header names for {key}\")\n",
    "\n",
    "async def main_3():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        # Divide the keys into batches (adjust the batch size as needed)\n",
    "        batch_size = 100\n",
    "        keys_batches = [list(characters_links.keys())[i:i + batch_size] for i in range(0, len(characters_links), batch_size)]\n",
    "\n",
    "        total_counter = 0\n",
    "        for keys_batch in keys_batches:\n",
    "            batch_results = await process_batch(keys_batch, session, total_counter, print_progress)\n",
    "            total_counter += sum(batch_results)\n",
    "            print(f\"Total fetched header names: {total_counter}\")\n",
    "\n",
    "await main_3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/characters_links_corrected.json', 'w') as json_file:\n",
    "    json.dump(characters_links)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
