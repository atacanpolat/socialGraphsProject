{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_arc_names(url):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find the element with class 'category-page__members'\n",
    "        category_members = soup.find('div', {'class': 'category-page__members'})\n",
    "\n",
    "        # Initialize all_content\n",
    "        all_content = \"\"\n",
    "\n",
    "        # Extract the content within the 'category-page__members' div\n",
    "        if category_members:\n",
    "            content = category_members.get_text()\n",
    "            # Append the content to the all_content string\n",
    "            all_content += content\n",
    "        else:\n",
    "            print(\"Div with class 'category-page__members' not found on the page.\")\n",
    "    else:\n",
    "        print(\"Failed to retrieve the page. Status code:\", response.status_code)\n",
    "\n",
    "    cleaned_string = re.sub(r'.\\t', '', all_content.replace('\\n', ' ')).replace('\\t', ' ')\n",
    "\n",
    "    # Split the input string by two or more whitespace characters using regular expression\n",
    "    name_list = re.split(r'\\s{2,}', cleaned_string)    \n",
    "\n",
    "    # Filter out any empty strings\n",
    "    name_list = [name.strip() for name in name_list if name.strip()]    \n",
    "\n",
    "    names_list = []\n",
    "    for name in name_list:\n",
    "        name = name.replace(' ', '_')\n",
    "        names_list.append(name)\n",
    "\n",
    "    return names_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Plot_of_Naruto', 'Academy_Entrance_Arc', 'Akatsuki_Suppression_Mission', 'Ao_Arc', 'Bikōchū_Search_Mission', \"Birth_of_the_Ten-Tails'_Jinchūriki\", \"Boruto's_Return_Arc\", 'Buried_Gold_Excavation_Mission', 'Byakuya_Gang_Arc', 'Childhood', 'Chūnin_Exams_(Arc)', 'Chōchō_Arc', 'Chūnin_Re-Examination_Arc', \"Code's_Assault_Arc\", 'Cursed_Warrior_Extermination_Mission', 'Kurosuki_Family_Removal_Mission', 'Kaguya_Ōtsutsuki_Strikes', 'Kaima_Capture_Mission', 'Kakashi_Gaiden', \"Kakashi's_Anbu_Arc:_The_Shinobi_That_Lives_in_the_Darkness\", 'Kara_Actuation_Arc', 'Kawaki_&_Himawari_Academy_Arc', 'Kawaki_Arc', 'Kazekage_Rescue_Mission', 'Konoha_Crush_(Arc)', 'Konoha_Hiden:_The_Perfect_Day_for_a_Wedding_(Arc)', 'Konoha_Plans_Recapture_Mission', \"Konohamaru's_Love_Arc\", 'Fated_Battle_Between_Brothers', 'Five_Kage_Summit_(Arc)', 'Fourth_Shinobi_World_War:_Climax', 'Fourth_Shinobi_World_War:_Confrontation', 'Fourth_Shinobi_World_War:_Countdown', 'Gantetsu_Escort_Mission', 'Genin_Mission_Arc', 'Gosunkugi_Capture_Mission', 'Graduation_Exams_Arc', 'Great_Sea_Battle_of_Kirigakure_Arc', \"In_Naruto's_Footsteps:_The_Friends'_Paths\", 'Itachi_Pursuit_Mission', 'Itachi_Shinden_Book:_Light_and_Darkness', 'Jiraiya_Shinobi_Handbook:_The_Tale_of_Naruto_the_Hero', 'Jūgo_Arc', 'Labyrinth_Game_Arc', 'Land_of_Rice_Fields_Investigation_Mission', 'Land_of_Tea_Escort_Mission', 'Menma_Memory_Search_Mission', \"Mitsuki's_Disappearance_Arc\", 'Mizuki_Tracking_Mission', 'Mujina_Bandits_Arc', 'Naruto_Gaiden:_The_Seventh_Hokage_and_the_Scarlet_Spring', 'Omnipotence_Arc', 'One-Tail_Escort_Arc', \"Pain's_Assault_(Arc)\", 'Paradise_Life_on_a_Boat', 'Parent_and_Child_Day_Arc', 'Past_Arc:_The_Locus_of_Konoha', 'Peddlers_Escort_Mission', 'Power_(Arc)', 'Prologue_—_Land_of_Waves', 'Sasuke_Recovery_Mission', 'Sasuke_Retsuden_Arc', 'Sasuke_Retsuden:_The_Uchiha_Descendants_and_the_Heavenly_Stardust_(manga)', 'Sasuke_Shinden:_Book_of_Sunrise_(Arc)', 'School_Trip_Arc', 'Search_for_Tsunade', 'Shikamaru_Hiden:_A_Cloud_Drifting_in_Silent_Darkness_(Arc)', 'Six-Tails_Unleashed', 'Star_Guard_Mission', 'Steam_Ninja_Scrolls_Arc', 'Sunagakure_Support_Mission', 'Tale_of_Jiraiya_the_Gallant', 'Tenchi_Bridge_Reconnaissance_Mission', 'Third_Great_Beast_Arc', \"Three-Tails'_Appearance\", 'Time_Slip_Arc', 'Twelve_Guardian_Ninja_(Arc)', 'Versus_Momoshiki_Arc', 'Yakumo_Kurama_Rescue_Mission']\n"
     ]
    }
   ],
   "source": [
    "arc_url = 'https://naruto.fandom.com/wiki/Category:Arcs'\n",
    "arc_names_list = scrape_arc_names(arc_url)\n",
    "\n",
    "print(arc_names_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_arc_episodes(url):\n",
    "    \n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find the span tag with the specified text and class\n",
    "        title_span = soup.find('span', class_='mw-headline', string='Episodes')\n",
    "\n",
    "        if title_span:\n",
    "    \n",
    "            # Find the corresponding table by searching for the closest table tag after the title\n",
    "            table = title_span.find_next('table', class_='box table colored bordered innerbordered style-basic')\n",
    "\n",
    "            if table:\n",
    "                # Initialize lists to store data\n",
    "                episode_data = []\n",
    "\n",
    "                # Loop through rows in the table\n",
    "                for row in table.find_all('tr')[1:]:  # Skip the header row\n",
    "                    # Extract data from each column in the row\n",
    "                    columns = row.find_all('td')\n",
    "\n",
    "                    # Assuming the structure is consistent, extract data from each column\n",
    "                    episode_number = row.find('th').get_text(strip=True)\n",
    "\n",
    "                    # Check if anchor tag is present before trying to get its text\n",
    "                    episode_title_tag = columns[0].find('a')\n",
    "                    episode_title = episode_title_tag.get_text(strip=True).replace(' ', '_') if episode_title_tag else None\n",
    "\n",
    "                    # Append data to the list of dictionaries\n",
    "                    episode_data.append({\n",
    "                        'Episode Number': episode_number,\n",
    "                        'Episode Title': episode_title,\n",
    "                    })\n",
    "\n",
    "                return episode_data\n",
    "\n",
    "            else:\n",
    "                print(f\"Table not found after the title Episode.\")\n",
    "                return None\n",
    "\n",
    "        else:\n",
    "            print(f\"Title with text 'Episode' not found on the page.\")\n",
    "            return None\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title with text 'Episode' not found on the page.\n",
      "Failed to retrieve episode data for the arc Plot_of_Naruto\n",
      "Title with text 'Episode' not found on the page.\n",
      "Failed to retrieve episode data for the arc Boruto's_Return_Arc\n",
      "Title with text 'Episode' not found on the page.\n",
      "Failed to retrieve episode data for the arc Omnipotence_Arc\n",
      "Title with text 'Episode' not found on the page.\n",
      "Failed to retrieve episode data for the arc Sasuke_Retsuden:_The_Uchiha_Descendants_and_the_Heavenly_Stardust_(manga)\n"
     ]
    }
   ],
   "source": [
    "url_base = 'https://naruto.fandom.com/wiki/'\n",
    "\n",
    "arc_episodes_dict = {}\n",
    "for arc in arc_names_list:\n",
    "    \n",
    "    url = url_base + arc\n",
    "    arc_episodes = scrape_arc_episodes(url)\n",
    "    \n",
    "    if arc_episodes:\n",
    "       arc_episodes_dict[arc] = arc_episodes\n",
    "    else:\n",
    "        print(f'Failed to retrieve episode data for the arc {arc}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save arc episodes data to json file\n",
    "with open('./data/arc_episodes_data.json', 'w') as json_file:\n",
    "    json.dump(arc_episodes_dict, json_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
