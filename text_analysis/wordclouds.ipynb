{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/lukasjonsson/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lukasjonsson/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download WordNet data (if not already done)\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to clean and preprocess text\n",
    "def clean_and_preprocess_text(raw_text):\n",
    "    # Remove Wikipedia markup, including '== ... ==' sections\n",
    "    text = re.sub(r'\\[\\[(.*?)\\]\\]', '', raw_text)  # Remove internal links\n",
    "    text = re.sub(r'<(.*?)>', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'==[^=]*==', '', text)  # Remove '==' sections\n",
    "\n",
    "    # Normalize whitespace\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Lowercasing\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "\n",
    "    # Remove special characters and punctuation\n",
    "    tokens = [re.sub(r'[^a-zA-Z0-9]', '', token) for token in tokens if token.isalnum()]\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Define paths to your West Coast and East Coast folders\n",
    "east_coast_folder = \"/Users/lukasjonsson/Desktop/DTU/Kandidat/3. semester/Social graphs & interactions/Assignment 2/eastcoasttexts\"\n",
    "west_coast_folder = \"/Users/lukasjonsson/Desktop/DTU/Kandidat/3. semester/Social graphs & interactions/Assignment 2/westcoasttexts\"\n",
    "\n",
    "# Process West Coast folder\n",
    "west_coast_text = []  # To store the cleaned and preprocessed text\n",
    "for filename in os.listdir(west_coast_folder):\n",
    "    file_path = os.path.join(west_coast_folder, filename)\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        raw_text = file.read()\n",
    "        cleaned_tokens = clean_and_preprocess_text(raw_text)\n",
    "        west_coast_text.extend(cleaned_tokens)\n",
    "\n",
    "# Process East Coast folder\n",
    "east_coast_text = []  # To store the cleaned and preprocessed text\n",
    "for filename in os.listdir(east_coast_folder):\n",
    "    file_path = os.path.join(east_coast_folder, filename)\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        raw_text = file.read()\n",
    "        cleaned_tokens = clean_and_preprocess_text(raw_text)\n",
    "        east_coast_text.extend(cleaned_tokens)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
