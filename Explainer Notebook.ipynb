{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainer Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "1. What is your dataset?\n",
    "2. Why did you choose this/these particular dataset(s)?\n",
    "3. What was your goal for the end user's experience?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset for the Naruto network analysis project consists of information related to character interactions, affiliations, and events within the Naruto anime series. It includes data on characters, their attributes and key points, and their relation to other characters, synopsis of each episode, summary of the arcs, and key plot points involving various characters.\n",
    "\n",
    "The Naruto dataset was chosen due to its popularity and the complexity of its character universe. with over 1400 characters and 1000 episodes, and rich and shifting chracter interactions throughout the series, Naruto universe is an ideal subject for network analysis.\n",
    "\n",
    "Even though our analysis spans over a broader range of questions, our main problem of interest is whether it is possible to systematically find the interchanging importance of the characters and the temporal patterns that emerge throughout the episodes in the series by applying tools from network science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic stats. Let's understand the dataset better\n",
    "1. Write about your choices in data cleaning and preprocessing\n",
    "2. Write a short section that discusses the dataset stats (here you can recycle the work you did for Project Assignment A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Fetching and Preprocessing Choices for Naruto Network Analysis\n",
    "\n",
    "The main data source for the Naruto network analysis project is the Naruto Wiki Fandom page. The data extraction process involves web scraping using Beautiful Soup to gather information about the characters, their attributes, relationships with other characters, affiliations with villages, and involvement in episodes and arcs. The code and detailed information about the fetching and preprocessing of each type of data can be found in the notebooks under the folder named \"data_fetching.\"\n",
    "\n",
    "1. Web Scraping with Beautiful Soup:\n",
    "\n",
    "    Web scraping was employed to extract data from the Naruto Wiki Fandom page. Beautiful Soup is used for the parsing of the webpage's HTML structure.\n",
    "\n",
    "2. Characters:\n",
    "\n",
    "    The attributes of each character, such as name, role, abilities, and status, were extracted and organized. Furthermore, character's analysis on their respective wiki page was extracted for textual analysis. Cleaning involved handling missing or inconsistent information to ensure a uniform dataset.\n",
    "\n",
    "3. Village Affiliations:\n",
    "\n",
    "    Information about the villages each character is affiliated with was gathered. Cleaning addressed discrepancies in village names, ensuring consistency in the dataset.\n",
    "\n",
    "4. Episodes and Arcs:\n",
    "\n",
    "    Data related to character involvement in episodes and arcs were collected. This involved extracting episode lists and the haracters that participated in. Furthermore, synopsis of each episode is collected for textual analysis. .\n",
    "\n",
    "5. Handling Missing Data:\n",
    "\n",
    "    Missing data points were addressed by either imputing values based on available information or marking instances where data was unavailable. This ensures completeness and reliability in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools, theory and analysis. Describe the process of theory to insight\n",
    "1. Talk about how you've worked with text, including regular expressions, unicode, etc.\n",
    "2. Describe which network science tools and data analysis strategies you've used, how those network science measures work, and why the tools you've chosen are right for the problem you're solving.\n",
    "3. How did you use the tools to understand your dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tools, Theory, Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion. Think critically about your creation\n",
    "1. What went well?,\n",
    "2. What is still missing? What could be improved?, Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The comprehensive analysis of the ”Naruto” series utilizing\n",
    "statistical network analysis, community detection, sentiment\n",
    "analysis, and attribute-based partitioning has shed light on\n",
    "the intricate dynamics of character interactions. However, it\n",
    "is essential to acknowledge the limitations of this study and\n",
    "outline potential avenues for future research.\n",
    "\n",
    "#### Limitations\n",
    "\n",
    "- Edge Weight Subjectivity: The assignment of edge\n",
    "weights based on character co-occurrences introduces a\n",
    "level of subjectivity. While we experimented with different\n",
    "levels of thresholds, and concluded that a threshold level\n",
    "of 100 delivered the clearest visilibility of the, the choice\n",
    "of a threshold (e.g., 100) may impact the interpretation \n",
    "of character significance, and alternative threshold values\n",
    "could lead to different network structures. Furthermore,\n",
    "the decision to filter characters and connections based on\n",
    "an edge weight threshold of 100 may inadvertently exclude\n",
    "potentially meaningful interactions. Striking a balance\n",
    "between inclusivity and focus is challenging and requires\n",
    "careful consideration\n",
    "\n",
    "- Community Detection Sensitivity: Community detection algorithms, while powerful, are sensitive to parameter\n",
    "settings. The Louvain algorithm employed in this analysis\n",
    "may produce different results with variations in resolution\n",
    "parameters, potentially influencing community assignments.\n",
    "\n",
    "- Attribute-Based Partitioning Challenges: The manual partitioning of the network based on character attributes,\n",
    "such as village affiliation, introduces its own set of challenges.\n",
    "Ambiguities in character allegiance or the lack of observation\n",
    "of latent variables may affect the accuracy of the partitioning.\n",
    "\n",
    "- Sentiment Analysis Complexity: The sentiment analysis captures the emotional tone of both the characters and\n",
    "the episodes. However, the inherent complexity of language\n",
    "and context limits the granularity of sentiment scores and\n",
    "their respective interpretation, both on the character level,\n",
    "and but also on the episode level.\n",
    "\n",
    "#### Possible Future Work\n",
    "\n",
    "- Dynamic Edge Weight Thresholding: Future research\n",
    "could explore dynamic edge weight thresholds that adapt\n",
    "to the evolving narrative of the series. This approach\n",
    "may provide a more nuanced understanding of character\n",
    "significance over different story arcs.\n",
    "\n",
    "- Algorithm Sensitivity Analysis: Conducting a sensitivity analysis on community detection algorithms, including parameter variations, could enhance the robustness\n",
    "of partitioning results. Exploring alternative algorithms\n",
    "may provide additional insights into community structures.\n",
    "Furthermore, other advanced clustering algorithms on vector\n",
    "representations of the nodes and their respective attributes\n",
    "can be explored.\n",
    "\n",
    "- Attribute-Based Partitioning Refinement: Improving the accuracy of attribute-based partitioning involves refining character attribute annotations and considering additional\n",
    "attributes such as character roles or power levels. This could\n",
    "lead to more accurate community assignments. With help\n",
    "of statistical learning algorithms, the most important node\n",
    "attributes can be detected and explored.\n",
    "\n",
    "- Fine-Tuning Sentiment Analysis: Developing a more\n",
    "sophisticated sentiment analysis model that considers context\n",
    "and character relationships may yield richer insights into the\n",
    "emotional dynamics of the series. This could involve natural\n",
    "language processing techniques beyond word clouds. Neural\n",
    "networks for sequential data or other statistical learning\n",
    "algorithms for NLP can be employed to predict more accurate\n",
    "sentiment results.\n",
    "\n",
    "- Incorporating Episode Scripts: Incorporating a textual analysis of the actual scripts of each episode rather\n",
    "than their synopsis may deliver interesting insights about the\n",
    "character interactions and evolution of the series that are\n",
    "otherwise not detectable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contributions. Who did what?\n",
    "You should write (just briefly) which group member was the main responsible for which elements of the assignment. (I want you guys to understand every part of the assignment, but usually there is someone who took lead role on certain portions of the work. That’s what you should explain)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
